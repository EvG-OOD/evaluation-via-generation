{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bec27c09-7a2d-4d7c-bec6-5a45698b3dcb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ViT OOD in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf204053-7882-4487-a314-926b6ed3ae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import roc_btw_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d842eb4f-cbe3-46bf-aa35-56a0acb51178",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a504b25-1758-4b8a-982a-90db77d0d885",
   "metadata": {},
   "source": [
    "## dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6724ff1-c5a1-4e8d-a4a0-738a4c904ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cifar10 (/root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8766385936486bbe0cce530ebe411c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cifar100 (/root/.cache/huggingface/datasets/cifar100/cifar100/1.0.0/f365c8b725c23e8f0f8d725c3641234d9331cd2f62919d1381d1baa5b3ba3142)\n",
      "Found cached dataset cifar10 (/root/.cache/huggingface/datasets/cifar10/plain_text/1.0.0/447d6ec4733dddd1ce3bb577c7166b986eaa4c538dcd9e805ba61f35674a9de4)\n",
      "Found cached dataset svhn (/root/.cache/huggingface/datasets/svhn/cropped_digits/1.0.0/8e83fcbe6f6078438cd826c7acd29e3de8ee7db44657b535f6d3453235529a31)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_ds, test_ds = load_dataset('cifar10', split=['train', 'test'])\n",
    "splits = train_ds.train_test_split(test_size=0.1)\n",
    "train_ds = splits['train']\n",
    "val_ds = splits['test']\n",
    "cifar100_ds = load_dataset('cifar100', split='test');\n",
    "\n",
    "cifar10_train_ds = load_dataset('cifar10', split='train')\n",
    "\n",
    "svhn_ds = load_dataset('svhn', 'cropped_digits', split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7214fd1d-17ac-4c29-bf1a-291b5d70f679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'airplane',\n",
       " 1: 'automobile',\n",
       " 2: 'bird',\n",
       " 3: 'cat',\n",
       " 4: 'deer',\n",
       " 5: 'dog',\n",
       " 6: 'frog',\n",
       " 7: 'horse',\n",
       " 8: 'ship',\n",
       " 9: 'truck'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {id:label for id, label in enumerate(train_ds.features['label'].names)}\n",
    "label2id = {label:id for id,label in id2label.items()}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "571950af-37ef-47c4-915a-47f0a069cef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "\n",
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7776b0b-95a8-47f3-b28a-b2499fe82953",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import (CenterCrop, \n",
    "                                    Compose, \n",
    "                                    Normalize, \n",
    "                                    RandomHorizontalFlip,\n",
    "                                    RandomResizedCrop, \n",
    "                                    Resize, \n",
    "                                    ToTensor)\n",
    "\n",
    "normalize = Normalize(mean=feature_extractor.image_mean, std=feature_extractor.image_std)\n",
    "_train_transforms = Compose(\n",
    "        [\n",
    "            RandomResizedCrop(feature_extractor.size),\n",
    "            RandomHorizontalFlip(),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "_val_transforms = Compose(\n",
    "        [\n",
    "            Resize(feature_extractor.size),\n",
    "            CenterCrop(feature_extractor.size),\n",
    "            ToTensor(),\n",
    "            normalize,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def train_transforms(examples):\n",
    "    examples['pixel_values'] = [_train_transforms(image.convert(\"RGB\")) for image in examples['img']]\n",
    "    return examples\n",
    "\n",
    "def val_transforms(examples):\n",
    "    examples['pixel_values'] = [_val_transforms(image.convert(\"RGB\")) for image in examples['img']]\n",
    "    return examples\n",
    "\n",
    "def val_transforms_svhn(examples):\n",
    "    examples['pixel_values'] = [_val_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8846693-cecb-425e-aaf4-36bbbc72f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the transforms\n",
    "train_ds.set_transform(train_transforms)\n",
    "\n",
    "val_ds.set_transform(val_transforms)\n",
    "test_ds.set_transform(val_transforms)\n",
    "\n",
    "cifar100_ds.set_transform(val_transforms)\n",
    "cifar10_train_ds.set_transform(val_transforms)\n",
    "\n",
    "svhn_ds.set_transform(val_transforms_svhn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2d5f1cc-03d1-4b42-8fbb-d5d7efcfdef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k',\n",
    "                                                  num_labels=10,\n",
    "                                                  id2label=id2label,\n",
    "                                                  label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bda94542-7d13-4f00-ba29-a556f4fcaafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 03:56:09.816124: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "metric_name = \"accuracy\"\n",
    "run_name = f\"finetune-cifar-10-epoch15\"\n",
    "num_train_epoch = 15\n",
    "\n",
    "args = TrainingArguments(\n",
    "    run_name,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=num_train_epoch,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    logging_dir='logs',\n",
    "    remove_unused_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d76167e7-9093-4f23-839b-d98f46ca52bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25891/3889184740.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"accuracy\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "# import evaluate\n",
    "import numpy as np\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e4aef72-68cd-4bac-b717-cc5006d75ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "def collate_fn_cifar100(examples):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    labels = torch.tensor([example[\"fine_label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
    "\n",
    "train_dataloader = DataLoader(test_ds, collate_fn=collate_fn, batch_size=4)\n",
    "test_dataloader = DataLoader(test_ds, collate_fn=collate_fn, batch_size=4)\n",
    "cifar100_dataloader = DataLoader(cifar100_ds, collate_fn=collate_fn_cifar100, batch_size=4)\n",
    "cifar10_train_dataloader = DataLoader(cifar10_train_ds, collate_fn=collate_fn, batch_size=4)\n",
    "\n",
    "svhn_dataloader = DataLoader(svhn_ds, collate_fn=collate_fn, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9c0c901-4f28-4246-867b-ed6be5ad0918",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=feature_extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0db90437-e3dd-4e3b-a0f2-fc33f9baa77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/evg/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 45000\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5280\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mswyoon\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/opt/home3/swyoon/evaluation-via-generation/src/research/wandb/run-20221020_035618-2jep95d6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/swyoon/huggingface/runs/2jep95d6\" target=\"_blank\">finetune-cifar-10-epoch15</a></strong> to <a href=\"https://wandb.ai/swyoon/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/evg/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1764' max='5280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1764/5280 20:04 < 40:03, 1.46 it/s, Epoch 5.01/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.361212</td>\n",
       "      <td>0.979400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.884300</td>\n",
       "      <td>0.200316</td>\n",
       "      <td>0.983600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.374100</td>\n",
       "      <td>0.140144</td>\n",
       "      <td>0.987000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.374100</td>\n",
       "      <td>0.106923</td>\n",
       "      <td>0.988400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.288100</td>\n",
       "      <td>0.083366</td>\n",
       "      <td>0.989400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to finetune-cifar-10-epoch15/checkpoint-352\n",
      "Configuration saved in finetune-cifar-10-epoch15/checkpoint-352/config.json\n",
      "Model weights saved in finetune-cifar-10-epoch15/checkpoint-352/pytorch_model.bin\n",
      "Feature extractor saved in finetune-cifar-10-epoch15/checkpoint-352/preprocessor_config.json\n",
      "/opt/conda/envs/evg/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to finetune-cifar-10-epoch15/checkpoint-704\n",
      "Configuration saved in finetune-cifar-10-epoch15/checkpoint-704/config.json\n",
      "Model weights saved in finetune-cifar-10-epoch15/checkpoint-704/pytorch_model.bin\n",
      "Feature extractor saved in finetune-cifar-10-epoch15/checkpoint-704/preprocessor_config.json\n",
      "/opt/conda/envs/evg/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to finetune-cifar-10-epoch15/checkpoint-1056\n",
      "Configuration saved in finetune-cifar-10-epoch15/checkpoint-1056/config.json\n",
      "Model weights saved in finetune-cifar-10-epoch15/checkpoint-1056/pytorch_model.bin\n",
      "Feature extractor saved in finetune-cifar-10-epoch15/checkpoint-1056/preprocessor_config.json\n",
      "/opt/conda/envs/evg/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to finetune-cifar-10-epoch15/checkpoint-1408\n",
      "Configuration saved in finetune-cifar-10-epoch15/checkpoint-1408/config.json\n",
      "Model weights saved in finetune-cifar-10-epoch15/checkpoint-1408/pytorch_model.bin\n",
      "Feature extractor saved in finetune-cifar-10-epoch15/checkpoint-1408/preprocessor_config.json\n",
      "/opt/conda/envs/evg/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to finetune-cifar-10-epoch15/checkpoint-1760\n",
      "Configuration saved in finetune-cifar-10-epoch15/checkpoint-1760/config.json\n",
      "Model weights saved in finetune-cifar-10-epoch15/checkpoint-1760/pytorch_model.bin\n",
      "Feature extractor saved in finetune-cifar-10-epoch15/checkpoint-1760/preprocessor_config.json\n",
      "/opt/conda/envs/evg/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024ecc36-8809-49c2-a732-db933c8ebcf5",
   "metadata": {},
   "source": [
    "## Classification output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61adbfb7-3a92-4740-8a06-b1183485fb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = trainer.predict(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9594a66d-b8dd-4cee-8600-4145bf6cba83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "y_true = outputs.label_ids\n",
    "y_pred = outputs.predictions.argmax(1)\n",
    "\n",
    "labels = train_ds.features['label'].names\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(xticks_rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb49615-98d9-40f6-9384-7cb08c1c66ef",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5b14f4-2664-4420-9872-7d5c437dc4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f619a1-c2f5-42b8-9661-03ef80ee0e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logit(model, dl):\n",
    "    l_pred = []\n",
    "    for xx in tqdm(dl):\n",
    "        with torch.no_grad():\n",
    "            pred = model(xx['pixel_values'].cuda(), output_hidden_states=False)\n",
    "            # pred.logits\n",
    "            # pred.hidden_states\n",
    "        l_pred.append(pred.logits.detach().cpu())\n",
    "    return torch.cat(l_pred)\n",
    "\n",
    "def get_prelogit(model, dl):\n",
    "    l_pred = []\n",
    "    for xx in tqdm(dl):\n",
    "        with torch.no_grad():\n",
    "            out = model(xx['pixel_values'].cuda(), output_hidden_states=True)\n",
    "            out = out.hidden_states[-1][:,0,:]\n",
    "            out = model.vit.layernorm(out)\n",
    "        l_pred.append(out.detach().cpu())\n",
    "    return torch.cat(l_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e039f-ce60-4e77-af4f-f43008aa325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_pred = get_logit(model, test_dataloader)\n",
    "cifar100_pred = get_logit(model, cifar100_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e93bb6-c491-471b-8c90-1c336281292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_pred.shape, cifar10_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c84e05-9cc9-435c-80f7-27780cedc7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59263b22-8a6a-4340-8cc2-342bcaabf79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_msp = torch.max(softmax(cifar100_pred, dim=1),dim=1).values\n",
    "cifar10_msp = torch.max(softmax(cifar10_pred, dim=1),dim=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237b779d-15df-4225-b2e9-8c839e17df9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar100_msp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9394728-45c6-4b08-927b-3618b6c73ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_btw_arr(cifar10_msp, cifar100_msp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac578be-1aa6-40ae-be52-68603d4d610a",
   "metadata": {},
   "source": [
    "### Mahalanobis distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46373115-9323-4ed5-914b-dd72c150debe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prelogit = get_prelogit(model, cifar10_train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6e8413-24b1-4ee6-952b-0d7f4a4f7553",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(prelogit, f'{run_name}/prelogit.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249acb45-4100-4671-afe7-ce72f94d5354",
   "metadata": {},
   "outputs": [],
   "source": [
    "prelogit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea767ed5-cc72-46a6-a7f0-d0afb9666a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_train_label = torch.tensor([cifar10_train_ds[i]['label'] for i in range(len(cifar10_train_ds))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41e1fa9-151e-40b4-b08b-369fdbf9df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e9a9ed-c3da-435d-97a9-a8b5027f7e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''mahalanobis statistics computation'''\n",
    "l_mean = []\n",
    "l_outer = []\n",
    "for k in range(10):\n",
    "    subset_x = prelogit[cifar10_train_label == k]\n",
    "    subset_mean = torch.mean(subset_x, dim=0, keepdim=True)\n",
    "    # subset_outer = torch.cov(subset_x - subset_mean, correction=0) * len(subset_x)\n",
    "    v = subset_x - subset_mean\n",
    "    subset_outer = v.T.mm(v)\n",
    "    l_mean.append(subset_mean)\n",
    "    l_outer.append(subset_outer)\n",
    "pooled_cov = torch.sum(torch.stack(l_outer), dim=0) / len(prelogit)\n",
    "all_means = torch.stack(l_mean, dim=-1)\n",
    "invcov = torch.linalg.inv(pooled_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06bdce4-1194-4975-bfda-fb30e8d03331",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''relative mahalanobis statistics'''\n",
    "whole_mean = torch.mean(prelogit, dim=0, keepdim=True)\n",
    "v = prelogit - whole_mean\n",
    "whole_cov = v.T.mm(v) / len(prelogit)\n",
    "whole_invcov = torch.linalg.inv(whole_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0477c063-89c0-478d-b38d-7e47361e9060",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''save'''\n",
    "torch.save({'all_means': all_means,\n",
    "            'invcov': invcov,\n",
    "            'whole_mean': whole_mean,\n",
    "            'whole_invcov': whole_invcov}, f'{run_name}/maha-statistic.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047cbd43-6157-47ac-9dbd-4b27ffbe4e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2400cb-0873-45ee-a23d-cd3840da5b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pooled_cov[0,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaacba4-f882-42f0-9a49-2616576c08f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_cov[0,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdee33af-7787-4b57-809f-be67b5bbc011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "37ce4b8f-fbc8-4054-a5cd-c057ada32058",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = torch.tensor([[1,2], [3,4.]])\n",
    "# xx = torch.tensor([[1,2],])\n",
    "mean = torch.tensor([[0.1,0.]])\n",
    "invcov = torch.tensor([[1, 0.1], [0., 1.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1aa00f0b-ad15-4464-be62-0da7b653bb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.9900, 10.9700],\n",
       "        [11.1900, 25.5700]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz = xx - mean\n",
    "zz.mm(invcov).mm(zz.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "026bda4b-b1f4-44d5-8952-ca4bf486a777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.9900])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_maha(xx[[0]], mean.unsqueeze(-1), invcov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b9fcbe5c-b7a1-4be0-b636-cbc10963d23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25.5700])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_maha(xx[[1]], mean.unsqueeze(-1), invcov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f40e3aba-e45e-4b69-8229-11d95499abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = xx[:1].unsqueeze(-1) - mean\n",
    "op1 = torch.einsum('ijk,jl->ilk', z, invcov)\n",
    "op2 = torch.einsum('ijk,ijk->ik', op1, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2f3c16f3-a02f-45e5-b412-0359e6cc96b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9000, 1.0000],\n",
       "         [1.9000, 2.0000]]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1e2bd0a8-f9ac-42e6-bc38-301ae497b3c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9000, 1.0000],\n",
       "         [1.9000, 2.0000]],\n",
       "\n",
       "        [[2.9000, 3.0000],\n",
       "         [3.9000, 4.0000]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9aff253e-f3e8-47c8-864c-705870b9e7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.4200, 5.0000]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5b06281-b9ca-490b-8f39-088da2cc6186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 4.4200,  5.0000],\n",
       "        [23.6200, 25.0000]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d7572dd-5af6-4e1c-8a89-5ab4deac81ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9000, 0.8000],\n",
       "         [3.7000, 3.4000]],\n",
       "\n",
       "        [[2.9000, 2.8000],\n",
       "         [9.7000, 9.4000]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "27c54cd4-4d2f-43bb-93c7-601f858e947d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.8400,  6.7600],\n",
       "        [46.2400, 43.5600]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50d45ca6-dc39-4669-8c10-c5e15072c2ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected size for first two dimensions of batch2 tensor to be: [2, 2] but got: [1, 2].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minvcov\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected size for first two dimensions of batch2 tensor to be: [2, 2] but got: [1, 2]."
     ]
    }
   ],
   "source": [
    "torch.bmm(z, invcov.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be967425-24d4-4949-b4bb-5c5edaa09fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6311fd05-2287-47f0-a6cd-6e2a11ceac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_maha(xx, mean, invcov):\n",
    "    \"\"\"\n",
    "    mean: [1, D, K]\n",
    "    invcov: [D, D]\n",
    "    \"\"\"\n",
    "    z = xx.unsqueeze(-1) - mean\n",
    "    op1 = torch.einsum('ijk,jl->ilk', z, invcov)\n",
    "    op2 = torch.einsum('ijk,ijk->ik', op1, z)  # [B, K]\n",
    "    \n",
    "    return torch.min(op2, dim=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f01ac0bf-a4a8-4f48-97a3-4b5ba562c79f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(l_score)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# forward_maha(subset_x, all_means, invcov)\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m cifar10_maha \u001b[38;5;241m=\u001b[39m forward_maha_dl(\u001b[43mmodel\u001b[49m, test_dataloader, all_means, invcov)\n\u001b[1;32m     27\u001b[0m cifar100_maha \u001b[38;5;241m=\u001b[39m forward_maha_dl(model, cifar100_dataloader, all_means, invcov)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "def forward_maha(xx, mean, invcov):\n",
    "    \"\"\"\n",
    "    mean: [1, D, K]\n",
    "    invcov: [D, D]\n",
    "    \"\"\"\n",
    "    z = xx.unsqueeze(-1) - mean\n",
    "    op1 = torch.einsum('ijk,jl->ilk', z, invcov)\n",
    "    op2 = torch.einsum('ijk,ijk->ik', op1, z)  # [B, K]\n",
    "    \n",
    "    return torch.min(op2, dim=1).values\n",
    "    \n",
    "def forward_maha_dl(model, dataloader, mean, invcov):\n",
    "    l_score = []\n",
    "    for xx in tqdm(dataloader):\n",
    "        \n",
    "        out = model(xx['pixel_values'].cuda(), output_hidden_states=True)\n",
    "        out = out.hidden_states[-1][:,0,:]\n",
    "        out = model.vit.layernorm(out)\n",
    "        \n",
    "        prelogit = out.detach().cpu()\n",
    "        score = forward_maha(prelogit, mean, invcov)\n",
    "        l_score.append(score)\n",
    "    return torch.cat(l_score)\n",
    "# forward_maha(subset_x, all_means, invcov)\n",
    "\n",
    "cifar10_maha = forward_maha_dl(model, test_dataloader, all_means, invcov)\n",
    "cifar100_maha = forward_maha_dl(model, cifar100_dataloader, all_means, invcov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5745ecd8-075f-4d2f-83a2-de5b389dfe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_btw_arr(cifar100_maha, cifar10_maha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925dea73-9f49-4a2d-8f3d-7c7e5297a93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "svhn_maha = forward_maha_dl(model, svhn_dataloader, all_means, invcov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2f3fe1-bf2c-471a-bedf-9e57d02b0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_btw_arr(svhn_maha, cifar10_maha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d876e1f-af53-403f-93a3-83dafb72ff0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = np.searchsorted(np.sort(cifar10_maha), svhn_maha)\n",
    "rank.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac07b2e-e741-4fef-8559-31a0a5fb8afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_rel_maha_dl(model, dataloader, mean, invcov, whole_mean, whole_invcov):\n",
    "    l_score = []\n",
    "    for xx in tqdm(dataloader):\n",
    "        \n",
    "        out = model(xx['pixel_values'].cuda(), output_hidden_states=True)\n",
    "        out = out.hidden_states[-1][:,0,:]\n",
    "        out = model.vit.layernorm(out)\n",
    "        \n",
    "        prelogit = out.detach().cpu()\n",
    "        maha = forward_maha(prelogit, mean, invcov)\n",
    "        rel_maha = forward_maha(prelogit, whole_mean, whole_invcov)\n",
    "        l_score.append(maha - rel_maha)\n",
    "    return torch.cat(l_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c8f97a-293f-4bf4-a0e0-e3d163293463",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_rel_maha = forward_rel_maha_dl(model, test_dataloader, all_means, invcov, whole_mean.unsqueeze(-1), whole_invcov)\n",
    "cifar100_rel_maha = forward_rel_maha_dl(model, cifar100_dataloader, all_means, invcov, whole_mean.unsqueeze(-1), whole_invcov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44a2267-2072-42c6-b47b-90921dd3d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_btw_arr(cifar100_rel_maha, cifar10_rel_maha)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:evg]",
   "language": "python",
   "name": "conda-env-evg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
